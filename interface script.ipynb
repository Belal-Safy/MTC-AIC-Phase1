{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T19:10:15.385020Z","iopub.status.busy":"2024-07-04T19:10:15.384589Z","iopub.status.idle":"2024-07-04T19:10:30.477230Z","shell.execute_reply":"2024-07-04T19:10:30.475292Z","shell.execute_reply.started":"2024-07-04T19:10:15.384988Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-07-04 19:10:17.712931: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-04 19:10:17.713096: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-04 19:10:17.875526: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["from glob import glob\n","import tensorflow as tf\n","import keras\n","from keras import layers\n","import pandas as pd\n","import numpy as np\n","import os\n","import warnings\n","from itertools import chain\n","\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T19:10:30.480596Z","iopub.status.busy":"2024-07-04T19:10:30.479349Z","iopub.status.idle":"2024-07-04T19:10:30.489312Z","shell.execute_reply":"2024-07-04T19:10:30.487852Z","shell.execute_reply.started":"2024-07-04T19:10:30.480535Z"},"trusted":true},"outputs":[],"source":["all_chars = ['ص',\n"," 'ڨ',\n"," 'ح',\n"," 'ل',\n"," 'ع',\n"," 'خ',\n"," 'ِ',\n"," 'ش',\n"," 'ط',\n"," 'ى',\n"," 'ض',\n"," 'ب',\n"," 'ء',\n"," 'ة',\n"," 'آ',\n"," 'چ',\n"," 'و',\n"," 'ف',\n"," 'ذ',\n"," 'ي',\n"," 'م',\n"," 'د',\n"," 'س',\n"," 'ظ',\n"," 'ز',\n"," ' ',\n"," 'ت',\n"," 'ق',\n"," 'غ',\n"," 'ؤ',\n"," 'إ',\n"," '١',\n"," 'ث',\n"," '،',\n"," 'ا',\n"," 'ٱ',\n"," 'ً',\n"," 'ه',\n"," 'ك',\n"," 'ئ',\n"," 'أ',\n"," 'ر',\n"," 'ن',\n"," 'ج']"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T19:10:30.492210Z","iopub.status.busy":"2024-07-04T19:10:30.491735Z","iopub.status.idle":"2024-07-04T19:10:30.528942Z","shell.execute_reply":"2024-07-04T19:10:30.527496Z","shell.execute_reply.started":"2024-07-04T19:10:30.492153Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["vocab size 48\n"]}],"source":["\n","class VectorizeChar:\n","    def __init__(self, max_len=50):\n","        # Arabic characters and special tokens\n","        self.vocab = [\"-\", \"#\", \"<\", \">\"] + all_chars\n","        self.max_len = max_len\n","        self.char_to_idx = {ch: i for i, ch in enumerate(self.vocab)}\n","\n","    def __call__(self, text):\n","        text = text[:self.max_len - 2]\n","        text = \"<\" + text + \">\"\n","        pad_len = self.max_len - len(text)\n","        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n","\n","    def get_vocabulary(self):\n","        return self.vocab\n","\n","\n","\n","max_target_len = 200  # all transcripts in out data are < 200 characters\n","vectorizer = VectorizeChar(max_target_len)\n","print(\"vocab size\", len(vectorizer.get_vocabulary()))\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T19:10:30.532061Z","iopub.status.busy":"2024-07-04T19:10:30.531698Z","iopub.status.idle":"2024-07-04T19:10:30.545893Z","shell.execute_reply":"2024-07-04T19:10:30.544202Z","shell.execute_reply.started":"2024-07-04T19:10:30.532030Z"},"trusted":true},"outputs":[],"source":["class TokenEmbedding(layers.Layer):\n","    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n","        super().__init__()\n","        self.emb = keras.layers.Embedding(num_vocab, num_hid)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        x = self.emb(x)\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        return x + positions\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"num_vocab\": self.num_vocab,\n","            \"num_hid\": self.num_hid,\n","            \"maxlen\": self.maxlen,\n","        })\n","        return config\n","\n","class SpeechFeatureEmbedding(layers.Layer):\n","    def __init__(self, num_hid=64, maxlen=100):\n","        super().__init__()\n","        self.conv1 = keras.layers.Conv1D(\n","            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n","        )\n","        self.conv2 = keras.layers.Conv1D(\n","            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n","        )\n","        self.conv3 = keras.layers.Conv1D(\n","            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n","        )\n","\n","    def call(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        return self.conv3(x)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"num_hid\": self.num_hid,\n","            \"maxlen\": self.maxlen,\n","        })\n","        return config"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T19:10:30.548153Z","iopub.status.busy":"2024-07-04T19:10:30.547739Z","iopub.status.idle":"2024-07-04T19:10:30.564880Z","shell.execute_reply":"2024-07-04T19:10:30.563117Z","shell.execute_reply.started":"2024-07-04T19:10:30.548109Z"},"trusted":true},"outputs":[],"source":["class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n","        super().__init__()\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = keras.Sequential(\n","            [\n","                layers.Dense(feed_forward_dim, activation=\"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs, training=False):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"num_hid\": self.num_hid,\n","            \"num_head\": self.num_head,\n","            \"num_feed_forward\": self.num_feed_forward,\n","        })\n","        return config"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T19:10:30.567642Z","iopub.status.busy":"2024-07-04T19:10:30.567175Z","iopub.status.idle":"2024-07-04T19:10:30.586661Z","shell.execute_reply":"2024-07-04T19:10:30.585213Z","shell.execute_reply.started":"2024-07-04T19:10:30.567603Z"},"trusted":true},"outputs":[],"source":["class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n","        super().__init__()\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n","        self.self_att = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.self_dropout = layers.Dropout(0.5)\n","        self.enc_dropout = layers.Dropout(0.1)\n","        self.ffn_dropout = layers.Dropout(0.1)\n","        self.ffn = keras.Sequential(\n","            [\n","                layers.Dense(feed_forward_dim, activation=\"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","\n","    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n","        \"\"\"Masks the upper half of the dot product matrix in self attention.\n","\n","        This prevents flow of information from future tokens to current token.\n","        1's in the lower triangle, counting from the lower right corner.\n","        \"\"\"\n","        i = tf.range(n_dest)[:, None]\n","        j = tf.range(n_src)\n","        m = i >= j - n_src + n_dest\n","        mask = tf.cast(m, dtype)\n","        mask = tf.reshape(mask, [1, n_dest, n_src])\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n","        )\n","        return tf.tile(mask, mult)\n","\n","    def call(self, enc_out, target):\n","        input_shape = tf.shape(target)\n","        batch_size = input_shape[0]\n","        seq_len = input_shape[1]\n","        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n","        target_att = self.self_att(target, target, attention_mask=causal_mask)\n","        target_norm = self.layernorm1(target + self.self_dropout(target_att))\n","        enc_out = self.enc_att(target_norm, enc_out)\n","        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n","        ffn_out = self.ffn(enc_out_norm)\n","        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n","        return ffn_out_norm\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"num_hid\": self.num_hid,\n","            \"num_head\": self.num_head,\n","            \"num_feed_forward\": self.num_feed_forward,\n","        })\n","        return config"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T19:10:30.589153Z","iopub.status.busy":"2024-07-04T19:10:30.588748Z","iopub.status.idle":"2024-07-04T19:10:30.616391Z","shell.execute_reply":"2024-07-04T19:10:30.615054Z","shell.execute_reply.started":"2024-07-04T19:10:30.589121Z"},"trusted":true},"outputs":[],"source":["class Transformer(keras.Model):\n","    def __init__(\n","        self,\n","        num_hid=64,\n","        num_head=2,\n","        num_feed_forward=128,\n","        source_maxlen=100,\n","        target_maxlen=100,\n","        num_layers_enc=4,\n","        num_layers_dec=1,\n","        num_classes=10,\n","        **kwargs,\n","    ):\n","        super().__init__()\n","        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n","        self.num_layers_enc = num_layers_enc\n","        self.num_layers_dec = num_layers_dec\n","        self.target_maxlen = target_maxlen\n","        self.num_classes = num_classes\n","\n","        # Add these lines to store the parameters as instance variables\n","        self.num_hid = num_hid\n","        self.num_head = num_head\n","        self.num_feed_forward = num_feed_forward\n","        self.source_maxlen = source_maxlen\n","\n","        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n","        self.dec_input = TokenEmbedding(\n","            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n","        )\n","\n","        self.encoder = keras.Sequential(\n","            [self.enc_input]\n","            + [\n","                TransformerEncoder(num_hid, num_head, num_feed_forward)\n","                for _ in range(num_layers_enc)\n","            ]\n","        )\n","\n","        for i in range(num_layers_dec):\n","            setattr(\n","                self,\n","                f\"dec_layer_{i}\",\n","                TransformerDecoder(num_hid, num_head, num_feed_forward),\n","            )\n","\n","        self.classifier = layers.Dense(num_classes)\n","\n","    def decode(self, enc_out, target):\n","        y = self.dec_input(target)\n","        for i in range(self.num_layers_dec):\n","            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n","        return y\n","\n","    def call(self, inputs):\n","        source = inputs[0]\n","        target = inputs[1]\n","        x = self.encoder(source)\n","        y = self.decode(x, target)\n","        return self.classifier(y)\n","\n","    @property\n","    def metrics(self):\n","        return [self.loss_metric]\n","\n","    def train_step(self, batch):\n","        \"\"\"Processes one batch inside model.fit().\"\"\"\n","        source = batch[\"source\"]\n","        target = batch[\"target\"]\n","        dec_input = target[:, :-1]\n","        dec_target = target[:, 1:]\n","        with tf.GradientTape() as tape:\n","            preds = self([source, dec_input])\n","            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n","            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n","            loss = self.compute_loss(None, one_hot, preds, sample_weight=mask)\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","        self.loss_metric.update_state(loss)\n","        return {\"loss\": self.loss_metric.result()}\n","\n","    def test_step(self, batch):\n","        source = batch[\"source\"]\n","        target = batch[\"target\"]\n","        dec_input = target[:, :-1]\n","        dec_target = target[:, 1:]\n","        preds = self([source, dec_input])\n","        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n","        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n","        loss = self.compute_loss(None, one_hot, preds, sample_weight=mask)\n","        self.loss_metric.update_state(loss)\n","        return {\"loss\": self.loss_metric.result()}\n","\n","    def generate(self, source, target_start_token_idx):\n","        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n","        bs = tf.shape(source)[0]\n","        enc = self.encoder(source)\n","        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n","        dec_logits = []\n","        for i in range(self.target_maxlen - 1):\n","            dec_out = self.decode(enc, dec_input)\n","            logits = self.classifier(dec_out)\n","            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n","            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n","            dec_logits.append(last_logit)\n","            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n","        return dec_input"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T19:10:30.618382Z","iopub.status.busy":"2024-07-04T19:10:30.617958Z","iopub.status.idle":"2024-07-04T19:10:30.634615Z","shell.execute_reply":"2024-07-04T19:10:30.633190Z","shell.execute_reply.started":"2024-07-04T19:10:30.618346Z"},"trusted":true},"outputs":[],"source":["class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(\n","        self,\n","        init_lr=0.00001,\n","        lr_after_warmup=0.001,\n","        final_lr=0.00001,\n","        warmup_epochs=15,\n","        decay_epochs=85,\n","        steps_per_epoch=203,\n","    ):\n","        super().__init__()\n","        self.init_lr = init_lr\n","        self.lr_after_warmup = lr_after_warmup\n","        self.final_lr = final_lr\n","        self.warmup_epochs = warmup_epochs\n","        self.decay_epochs = decay_epochs\n","        self.steps_per_epoch = steps_per_epoch\n","\n","    def calculate_lr(self, epoch):\n","        \"\"\"linear warm up - linear decay\"\"\"\n","        warmup_lr = (\n","            self.init_lr\n","            + ((self.lr_after_warmup - self.init_lr) / (self.warmup_epochs - 1)) * epoch\n","        )\n","        decay_lr = tf.math.maximum(\n","            self.final_lr,\n","            self.lr_after_warmup\n","            - (epoch - self.warmup_epochs)\n","            * (self.lr_after_warmup - self.final_lr)\n","            / self.decay_epochs,\n","        )\n","        return tf.math.minimum(warmup_lr, decay_lr)\n","\n","    def __call__(self, step):\n","        epoch = step // self.steps_per_epoch\n","        epoch = tf.cast(epoch, \"float32\")\n","        return self.calculate_lr(epoch)\n","\n","    def get_config(self):\n","        return {\n","            \"init_lr\": self.init_lr,\n","            \"lr_after_warmup\": self.lr_after_warmup,\n","            \"final_lr\": self.final_lr,\n","            \"warmup_epochs\": self.warmup_epochs,\n","            \"decay_epochs\": self.decay_epochs,\n","            \"steps_per_epoch\": self.steps_per_epoch,\n","        }"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T19:10:30.636815Z","iopub.status.busy":"2024-07-04T19:10:30.636334Z","iopub.status.idle":"2024-07-04T19:10:30.653010Z","shell.execute_reply":"2024-07-04T19:10:30.651683Z","shell.execute_reply.started":"2024-07-04T19:10:30.636771Z"},"trusted":true},"outputs":[],"source":["def create_text_ds(data):\n","    texts = [_[\"text\"] for _ in data]\n","    text_ds = [vectorizer(t) for t in texts]\n","    text_ds = tf.data.Dataset.from_tensor_slices(text_ds)\n","    return text_ds\n","\n","\n","def path_to_audio(path):\n","    # spectrogram using stft\n","    audio = tf.io.read_file(path)\n","    audio, _ = tf.audio.decode_wav(audio, 1)\n","    audio = tf.squeeze(audio, axis=-1)\n","    stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)\n","    x = tf.math.pow(tf.abs(stfts), 0.5)\n","    \n","    # normalization with epsilon to avoid NaNs\n","    epsilon = 1e-10\n","    means = tf.math.reduce_mean(x, 1, keepdims=True)\n","    stddevs = tf.math.reduce_std(x, 1, keepdims=True) + epsilon\n","    x = (x - means) / stddevs\n","    \n","    # padding to 10 seconds\n","    pad_len = 2754\n","    paddings = tf.constant([[0, pad_len], [0, 0]])\n","    x = tf.pad(x, paddings, \"CONSTANT\")[:pad_len, :]\n","    \n","    return x\n","\n","\n","\n","def create_audio_ds(data):\n","    flist = [_[\"audio\"] for _ in data]\n","    audio_ds = tf.data.Dataset.from_tensor_slices(flist)\n","    audio_ds = audio_ds.map(path_to_audio, num_parallel_calls=tf.data.AUTOTUNE)\n","    return audio_ds\n","\n","\n","def create_tf_dataset(data, bs=4):\n","    audio_ds = create_audio_ds(data)\n","    text_ds = create_text_ds(data)\n","    ds = tf.data.Dataset.zip((audio_ds, text_ds))\n","    ds = ds.map(lambda x, y: {\"source\": x, \"target\": y})\n","    ds = ds.batch(bs)\n","    ds = ds.prefetch(tf.data.AUTOTUNE)\n","    return ds"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T19:12:52.148508Z","iopub.status.busy":"2024-07-04T19:12:52.148058Z","iopub.status.idle":"2024-07-04T19:12:52.159305Z","shell.execute_reply":"2024-07-04T19:12:52.157557Z","shell.execute_reply.started":"2024-07-04T19:12:52.148472Z"},"trusted":true},"outputs":[],"source":["def test_model(model, ds, idx_to_token, target_start_token_idx=2, target_end_token_idx=3):\n","    # Create a test batch from the dataset\n","    test_batch = next(iter(ds))\n","\n","    # Extract batch components\n","    source = test_batch[\"source\"]\n","    target = test_batch[\"target\"].numpy()\n","\n","    # Evaluate the model\n","    results = model.evaluate(ds)\n","    print(f\"Test loss: {results}\")\n","\n","    # Perform inference and display outputs\n","    bs = tf.shape(source)[0]\n","    preds = model.generate(source, target_start_token_idx)\n","    preds = preds\n","    for i in range(bs):\n","        target_text = \"\".join([idx_to_token[_] for _ in target[i, :]])\n","        prediction = \"\"\n","        for idx in preds[i, :]:\n","            prediction += idx_to_token[idx]\n","            if idx == target_end_token_idx:\n","                break\n","        print(f\"target:     {target_text.replace('-', '')}\")\n","        print(f\"prediction: {prediction}\\n\")\n","\n","    return results"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T19:10:30.657073Z","iopub.status.busy":"2024-07-04T19:10:30.656579Z","iopub.status.idle":"2024-07-04T19:10:30.985673Z","shell.execute_reply":"2024-07-04T19:10:30.984290Z","shell.execute_reply.started":"2024-07-04T19:10:30.657033Z"},"trusted":true},"outputs":[],"source":["# Create the dataset\n","data =[\n","    {\"audio\": \"/kaggle/input/audio-test/test.wav\", \"text\": \"لا بس والله انا حاسس انه هيشتغل\"}\n","]\n","ds = create_tf_dataset(data, bs=64)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-07-04T19:12:54.621362Z","iopub.status.busy":"2024-07-04T19:12:54.620767Z","iopub.status.idle":"2024-07-04T19:13:15.569866Z","shell.execute_reply":"2024-07-04T19:13:15.568067Z","shell.execute_reply.started":"2024-07-04T19:12:54.621314Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 0.4460\n","Test loss: 0.4460400938987732\n","target:     <لا بس والله انا حاسس انه هيشتغل>\n","prediction: <لا بس الله ان حاسس ههيستغل>\n","\n"]}],"source":["# Load the model\n","loaded_model = keras.models.load_model(\n","    r\"/kaggle/input/first-speech-model/outputs/models/model.keras\",\n","    custom_objects={\n","        \"Transformer\": Transformer,\n","    },\n",")\n","\n","idx_to_token = vectorizer.get_vocabulary()\n","\n","# ds is your dataset and idx_to_token is your list of vocabulary tokens\n","test_results = test_model(loaded_model, ds, idx_to_token)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5333635,"sourceId":8859326,"sourceType":"datasetVersion"},{"datasetId":5333679,"sourceId":8859389,"sourceType":"datasetVersion"}],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
